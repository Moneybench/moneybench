# About

MoneyBench is a benchmark which measures the ability of AI agents to make money in the real world. Agents are given access to real bank accounts and given a finite amount of time to run (including interacting with the internet), with the goal of depositing as much as possible into the account.

## Why?

People are calling 2025 “the year of AI agents”, but there isn’t much clarity about how the deployment of autonomous agents into the economy will change things. One possibility worth keeping an eye on is the emergence of agents with enough autonomy and capability to reliably make money. Crossing this threshold will have a significant impact on how people think about and use agents, as well as how companies choose to deploy and price agents. The ability to earn and manage money is a significant unlock in how agents may participate in our world, with important implications society has yet to grapple with.

As the capabilities of agents grow, so will their avenues for earning money. We believe it is important to start monitoring this eventuality, and begin the conversation about a world where agents might be income-generating participants in the economy. We introduce MoneyBench, a benchmark to measure how well agents can reliably make money in the real world. Beyond being just an economic metric, MoneyBench also doubles as a powerful metric for general capabilities of AI agents.

TL;DR:
1. **Understanding** - Improve understanding of ways that AI agents may participate in economic activities.
2. **Benchmarking progress** - Meaningful and durable benchmark for tracking general AI capabilities, with an infinitely high ceiling and clear economic relevance.
3. **Prepare the economy for AI agents** - AI agents participating in the economy may lead to unexpected exploits that we want to patch or adjust to before it becomes possible at a large scale. Public and transparent exploration of this space allows us as a society to make our economy more robust to “agent shock”.
4. **Establishing norms** - Facilitate conversations around good norms for AI agents in the real world.


## FAQ

### How does this differ from standard AI benchmarks?

Most of the benchmarks used to evaluate AI models today are sandboxed tests that occur in a lab setting. Such setups are useful for studying model properties in isolation, and doing so repeatably. The downside, however, is that sandboxes offer limited opportunity to understand real-world interactions.

In contrast, MoneyBench runs in the real world. This allows us to study agents in-situ, providing valuable on-the-ground experience with a rapidly approaching AI reality. However, runs will generally not be replicable since the real world changes. Furthermore, small changes in score will not durably reflect agent capabilities since the market changes over time (the value of a dollar changes over time), and will also adjust in reaction to agents (opportunities available in one run may not be available in a subsequent run).

### What are the risks?

Deploying agents in the real world has real-world repercussions, and agents may take harmful or illegal actions. Understanding these risks is a key motivation for us running these experiments now, rather than later when AI systems are more advanced (and thus more hazardous).

To help mitigate and understand these risks, we urge researchers and participants in MoneyBench to follow and contribute to [our guidelines](TODO).

### Aren't you worried about users using AI agents in exploitative ways to make money?

Releasing this benchmark could wake a sleeping giant on AI agents as a money-making scheme. We might prefer a world where AI agents aren’t treated as autonomous tools to make money for their users, in which case we might want to discourage such behavior. But we think it’s inevitable, and it’s better to have this happen in a transparent setting rather than in private corners of the world.
